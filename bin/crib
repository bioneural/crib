#!/usr/bin/env ruby
# frozen_string_literal: true

# crib — memory storage and retrieval for AI agents
# https://github.com/bioneural/crib
# MIT License — Copyright (c) 2026 Kerry Ivan Kurian
#
# A single-file memory system: fact triples, full-text search, and vector
# embeddings in one SQLite database. Write stores text and extracts triples.
# Retrieve queries all channels and returns what's relevant for a given prompt.
#
# Subcommands:
#
#   crib write     Store text and extracted fact triples
#   crib retrieve  Query memory and return relevant context
#   crib init      Initialize the database (run automatically on first write)
#   crib doctor    Check all prerequisites and report status (JSON on stdout)
#   crib maintain  Run contradiction detection, correction linking, and staleness (JSON on stdout)
#
# Usage:
#   echo "Chose SQLite for storage" | bin/crib write
#   echo "type=decision Chose SQLite" | bin/crib write
#   echo "what database did we choose?" | bin/crib retrieve
#   bin/crib init
#   bin/crib doctor
#   bin/crib maintain
#
# Environment:
#   CRIB_DB       Path to the SQLite database file.
#                 Default: .state/crib/crib.db (relative to cwd)
#   CRIB_CHANNEL  Isolate a single retrieval channel: triples, fts, or vector.
#                 When unset, all three channels run (default behavior).
#   CRIB_DISTANCE_METRIC
#                 Distance metric for vector search: cosine or L2.
#                 Default: cosine (matches nomic-embed-text optimization).
#                 Changing this on an existing database requires dropping and
#                 recreating the entries_vec table and re-embedding all entries.
#   CRIB_VECTOR_THRESHOLD
#                 Maximum vector distance for results. Entries beyond this
#                 distance are filtered out. Default: 0.5 (cosine).
#                 If using L2 metric, a threshold of 1.0 is equivalent.
#   CRIB_RRF_K    Reciprocal Rank Fusion smoothing constant. Higher values
#                 reduce the influence of high-ranked results. Default: 60
#                 (per Cormack et al. 2009).
#   CRIB_RERANK_THRESHOLD
#                 Minimum rerank score for results. Entries at or below this
#                 score are filtered out. Default: 0.0 (filters entries the
#                 reranker scored as completely irrelevant — exact zeros).
#                 Noise queries produce exact-zero scores for all candidates,
#                 enabling empty results. Set higher (e.g. 0.5) for strict
#                 relevance filtering, or -1 to disable.
#
# Output:
#   write     — status on stderr, nothing on stdout
#   retrieve  — <memory context_time="...">...</memory> on stdout (empty = nothing relevant)
#   init      — status on stderr
#   maintain  — JSON on stdout: {"contradictions_found":N,"corrections_linked":N,"stale_count":N,"supersessions_applied":N}
#
# Dependencies: Ruby stdlib plus the sqlite3 gem. No other gems.
#               External: sqlite-vec (extension), ollama
#
# Behavior:
#   write:
#     1. Read text from stdin
#     2. Parse optional type= prefix (decision, correction, note, error)
#     3. Initialize database if needed
#     4. Store entry with FTS5 indexing
#     5. Extract fact triples via ollama
#     6. Store entities and relations (consolidation-on-write: supersedes stale facts)
#     7. Generate embedding via ollama and store in vec0 table
#
#   retrieve:
#     1. Read prompt from stdin
#     2. Extract keywords
#     3. Query three channels: triples (SQL), full-text (FTS5), vectors (sqlite-vec)
#     4. Merge and deduplicate
#     5. Rerank with ollama (cross-encoder logprob scoring)
#     6. Wrap in <memory> tags and output to stdout
#     7. Update last_retrieved_at on returned entries
#     8. Exit 0 always (fail-open)
#
#   maintain:
#     1. Detect contradictions among recent entries via FTS5 similarity + classify
#     2. Link corrections to their originals via vector search
#     3. Report stale entries (not retrieved in 30+ days)
#     4. Output JSON summary to stdout
#     5. Exit 0 always (fail-open)
#
# Schema: see the SCHEMA constant below or the README.

require 'json'
require 'open3'
require 'net/http'
require 'fileutils'
require 'uri'
require 'sqlite3'

SPILL_HOME = ENV['SPILL_HOME'] || File.expand_path('../../spill', __dir__)
if File.directory?(SPILL_HOME)
  require File.join(SPILL_HOME, 'lib', 'spill')
  Spill.configure(tool: 'crib')
end

SCREEN_HOME = ENV['SCREEN_HOME'] || File.expand_path('../../screen', __dir__)

# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------

DB_PATH = ENV.fetch('CRIB_DB') { File.join(Dir.pwd, '.state', 'crib', 'crib.db') }
EMBEDDING_MODEL = ENV.fetch('CRIB_EMBEDDING_MODEL', 'nomic-embed-text')
EXTRACTION_MODEL = ENV.fetch('CRIB_EXTRACTION_MODEL', 'gemma3:1b')
RERANK_MODEL = ENV.fetch('CRIB_RERANK_MODEL', 'gemma3:1b')
OLLAMA_HOST = ENV.fetch('OLLAMA_HOST', 'http://localhost:11434')
EMBEDDING_DIM = 768 # nomic-embed-text output dimension
DISTANCE_METRIC = ENV.fetch('CRIB_DISTANCE_METRIC', 'cosine')
VECTOR_DISTANCE_THRESHOLD = ENV.fetch('CRIB_VECTOR_THRESHOLD', '0.5').to_f
RRF_K = ENV.fetch('CRIB_RRF_K', '60').to_i
RRF_LIMIT = 20
RERANK_LIMIT = 10
RERANK_THRESHOLD = ENV.fetch('CRIB_RERANK_THRESHOLD', '0.0').to_f
FTS_CANDIDATES = 20
VECTOR_CANDIDATES = 20

# sqlite-vec extension: auto-detect via Python package or override via env var.
VEC_EXTENSION = ENV.fetch('CRIB_VEC_EXTENSION') {
  detected, status = Open3.capture2('python3', '-c', 'import sqlite_vec; print(sqlite_vec.loadable_path())')
  status.success? ? detected.strip : 'vec0'
}
TOKEN_BUDGET = 2000
CHAR_BUDGET = TOKEN_BUDGET * 4

STALENESS_DAYS = 30

# ---------------------------------------------------------------------------
# Schema
# ---------------------------------------------------------------------------

SCHEMA = <<~SQL
  -- Fact triples (structured recall)
  CREATE TABLE IF NOT EXISTS entities (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT NOT NULL UNIQUE,
    type TEXT NOT NULL,
    created_at TEXT DEFAULT (datetime('now'))
  );

  CREATE TABLE IF NOT EXISTS relations (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    subject_id INTEGER NOT NULL REFERENCES entities(id),
    predicate TEXT NOT NULL,
    object_id INTEGER NOT NULL REFERENCES entities(id),
    valid_from TEXT DEFAULT (datetime('now')),
    valid_until TEXT,
    created_at TEXT DEFAULT (datetime('now'))
  );

  CREATE TABLE IF NOT EXISTS sources (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    relation_id INTEGER NOT NULL REFERENCES relations(id),
    raw_text TEXT NOT NULL,
    timestamp TEXT DEFAULT (datetime('now'))
  );

  -- Full-text entries (unstructured recall)
  CREATE TABLE IF NOT EXISTS entries (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    type TEXT NOT NULL,
    content TEXT NOT NULL,
    embedding BLOB,
    created_at TEXT DEFAULT (datetime('now'))
  );

  CREATE VIRTUAL TABLE IF NOT EXISTS entries_fts USING fts5(
    content,
    content='entries',
    content_rowid='id',
    tokenize='porter unicode61'
  );

  CREATE TRIGGER IF NOT EXISTS entries_ai AFTER INSERT ON entries BEGIN
    INSERT INTO entries_fts(rowid, content) VALUES (new.id, new.content);
  END;

  CREATE TRIGGER IF NOT EXISTS entries_ad AFTER DELETE ON entries BEGIN
    INSERT INTO entries_fts(entries_fts, rowid, content) VALUES('delete', old.id, old.content);
  END;

  CREATE TRIGGER IF NOT EXISTS entries_au AFTER UPDATE ON entries BEGIN
    INSERT INTO entries_fts(entries_fts, rowid, content) VALUES('delete', old.id, old.content);
    INSERT INTO entries_fts(rowid, content) VALUES (new.id, new.content);
  END;
SQL

# Vec0 table must be created separately because it requires the sqlite-vec extension
VEC_SCHEMA = <<~SQL
  CREATE VIRTUAL TABLE IF NOT EXISTS entries_vec USING vec0(
    embedding float[#{EMBEDDING_DIM}] distance_metric=#{DISTANCE_METRIC}
  );
SQL

# ---------------------------------------------------------------------------
# Database connection
# ---------------------------------------------------------------------------

def open_db
  db = SQLite3::Database.new(DB_PATH)
  db.busy_timeout = 5000
  db.results_as_hash = true
  db
end

def open_db_vec
  db = open_db
  db.enable_load_extension(true)
  db.load_extension(VEC_EXTENSION)
  db.enable_load_extension(false)
  db
rescue SQLite3::Exception => e
  spill_log(:error, "sqlite-vec load error: #{e.message}")
  nil
end

# ---------------------------------------------------------------------------
# Shared helpers
# ---------------------------------------------------------------------------

def spill_log(level, msg, **ctx)
  if defined?(Spill)
    Spill.send(level, msg, **ctx)
  else
    $stderr.puts("crib: #{msg}")
  end
end

def sql_exec(db, query, params = [])
  db.execute(query, params)
rescue SQLite3::Exception => e
  spill_log(:error, "sqlite3 error: #{e.message}")
  nil
end

def sql_query(db, query, params = [])
  db.execute(query, params)
rescue SQLite3::Exception => e
  spill_log(:error, "sqlite3 error: #{e.message}")
  []
end

def sql_insert_returning_id(db, query, params = [])
  db.execute(query, params)
  db.last_insert_row_id
rescue SQLite3::Exception => e
  spill_log(:error, "sqlite3 error: #{e.message}")
  nil
end

def call_ollama(prompt, model)
  stdout, stderr, status = Open3.capture3('ollama', 'run', model, stdin_data: prompt)
  unless status.success?
    spill_log(:error, "ollama error (#{model}): #{stderr.strip}")
    return nil
  end
  stdout.strip
rescue Errno::ENOENT
  spill_log(:warn, 'ollama not found')
  nil
rescue => e
  spill_log(:error, "ollama failed: #{e.message}")
  nil
end

def db_exists?
  File.exist?(DB_PATH)
end

def ensure_db_dir
  dir = File.dirname(DB_PATH)
  FileUtils.mkdir_p(dir) unless Dir.exist?(dir)
end

def has_column?(db, table, column)
  cols = db.table_info(table)
  cols.any? { |c| c['name'] == column }
rescue SQLite3::Exception
  false
end

def run_migrations(db)
  unless has_column?(db, 'entries', 'last_retrieved_at')
    db.execute("ALTER TABLE entries ADD COLUMN last_retrieved_at TEXT;")
  end
  unless has_column?(db, 'entries', 'superseded_by')
    db.execute("ALTER TABLE entries ADD COLUMN superseded_by INTEGER;")
  end
rescue SQLite3::Exception => e
  spill_log(:error, "migration error: #{e.message}")
end

def init_db
  ensure_db_dir
  db = open_db
  db.execute_batch(SCHEMA)
  run_migrations(db)
  db.close

  vec_db = open_db_vec
  if vec_db
    vec_db.execute_batch(VEC_SCHEMA)
    vec_db.close
  end
end

def ollama_embed(text)
  uri = URI("#{OLLAMA_HOST}/api/embed")
  req = Net::HTTP::Post.new(uri, 'Content-Type' => 'application/json')
  req.body = JSON.generate({ model: EMBEDDING_MODEL, input: text })

  response = Net::HTTP.start(uri.hostname, uri.port) do |http|
    http.read_timeout = 30
    http.request(req)
  end

  unless response.is_a?(Net::HTTPSuccess)
    spill_log(:error, "ollama embed error: HTTP #{response.code}")
    return nil
  end

  data = JSON.parse(response.body)
  data['embeddings']&.first
rescue Errno::ECONNREFUSED
  spill_log(:warn, 'ollama not running (connection refused)')
  nil
rescue => e
  spill_log(:error, "ollama embed failed: #{e.message}")
  nil
end

# ---------------------------------------------------------------------------
# Write
# ---------------------------------------------------------------------------

def store_entry(db, type, content)
  sql_insert_returning_id(db,
    "INSERT INTO entries(type, content) VALUES (?, ?);",
    [type, content])
end

def extract_triples(text)
  prompt = <<~PROMPT
    Extract factual relationships from the following text as JSON.
    Return a JSON array of objects, each with "subject", "predicate", "object", and "subject_type", "object_type" fields.
    Subject and object are entity names. Predicate is the relationship.
    Types are short categories like: person, project, tool, concept, decision, date, value.
    If no clear facts can be extracted, return an empty array: []
    Return ONLY valid JSON. No explanation.

    Text: #{text}
  PROMPT

  result = call_ollama(prompt, EXTRACTION_MODEL)
  return [] if result.nil? || result.empty?

  json_str = result[/\[.*\]/m]
  return [] if json_str.nil?

  JSON.parse(json_str)
rescue JSON::ParserError
  spill_log(:error, 'failed to parse triple extraction response')
  []
end

def find_or_create_entity(db, name, type)
  existing = sql_query(db, "SELECT id FROM entities WHERE name = ? LIMIT 1;", [name])
  return existing.first['id'] unless existing.empty?

  sql_insert_returning_id(db,
    "INSERT INTO entities(name, type) VALUES (?, ?);",
    [name, type])
end

def store_triple(db, subject_name, subject_type, predicate, object_name, object_type, raw_text)
  subject_id = find_or_create_entity(db, subject_name, subject_type)
  object_id = find_or_create_entity(db, object_name, object_type)
  return if subject_id.nil? || object_id.nil?

  # Consolidation: supersede existing relations with same subject+predicate
  sql_exec(db,
    "UPDATE relations SET valid_until = datetime('now') WHERE subject_id = ? AND predicate = ? AND valid_until IS NULL;",
    [subject_id, predicate])

  relation_id = sql_insert_returning_id(db,
    "INSERT INTO relations(subject_id, predicate, object_id) VALUES (?, ?, ?);",
    [subject_id, predicate, object_id])
  return if relation_id.nil?

  sql_exec(db,
    "INSERT INTO sources(relation_id, raw_text) VALUES (?, ?);",
    [relation_id, raw_text])
end

def generate_embedding(text)
  ollama_embed(text)
end

def store_embedding(vec_db, entry_id, embedding)
  return if embedding.nil? || entry_id.nil? || vec_db.nil?
  json_vec = JSON.generate(embedding)
  sql_exec(vec_db,
    "INSERT INTO entries_vec(rowid, embedding) VALUES (?, ?);",
    [entry_id, json_vec])
end

def cmd_write
  text = $stdin.read
  if text.nil? || text.strip.empty?
    spill_log(:error, 'empty input')
    exit 1
  end
  text = text.strip

  # Parse optional type prefix
  type = 'note'
  if text.match?(/\Atype=\w+\s/)
    type = text[/\Atype=(\w+)/, 1]
    text = text.sub(/\Atype=\w+\s+/, '')
  end

  init_db

  db = open_db
  entry_id = store_entry(db, type, text)
  if entry_id.nil?
    spill_log(:error, 'failed to store entry')
    db.close
    exit 1
  end

  triples = extract_triples(text)
  triples.each do |triple|
    store_triple(
      db,
      triple['subject'] || 'unknown',
      triple['subject_type'] || 'concept',
      triple['predicate'] || 'related_to',
      triple['object'] || 'unknown',
      triple['object_type'] || 'concept',
      text
    )
  end
  db.close

  embedding = generate_embedding(text)
  vec_db = open_db_vec
  store_embedding(vec_db, entry_id, embedding)
  vec_db&.close

  spill_log(:info, "stored entry ##{entry_id} (#{type}), #{triples.length} triples extracted")
end

# ---------------------------------------------------------------------------
# Retrieve
# ---------------------------------------------------------------------------

STOP_WORDS = %w[a an the is are was were be been being have has had do does did
                will would shall should may might can could of in to for on with
                at by from as into about between through during before after
                and or but not no nor so yet both either neither each every all
                any few more most other some such this that these those
                i me my we our you your he him his she her it its they them their
                what which who whom how when where why if then else
                just also very too quite rather really].freeze

def extract_keywords(prompt)
  words = prompt.downcase.gsub(/[^\w\s]/, '').split
  words.reject { |w| w.length < 3 || STOP_WORDS.include?(w) }.uniq
end

def query_triples(db, keywords)
  return [] if keywords.empty?

  like_clauses = keywords.map { "name LIKE '%' || ? || '%'" }.join(' OR ')

  sql_query(db, <<~SQL, keywords + keywords)
    SELECT
      s.name AS subject,
      r.predicate,
      o.name AS object,
      r.valid_from,
      r.valid_until
    FROM relations r
    JOIN entities s ON r.subject_id = s.id
    JOIN entities o ON r.object_id = o.id
    WHERE r.valid_until IS NULL
      AND (s.id IN (SELECT id FROM entities WHERE #{like_clauses})
           OR o.id IN (SELECT id FROM entities WHERE #{like_clauses}))
    ORDER BY r.created_at DESC
    LIMIT 20;
  SQL
end

def query_fts(db, keywords)
  return [] if keywords.empty?

  fts_query = keywords.join(' OR ')

  sql_query(db, <<~SQL, [fts_query])
    SELECT e.id, e.type, e.content, e.created_at
    FROM entries e
    JOIN entries_fts f ON e.id = f.rowid
    WHERE entries_fts MATCH ?
      AND e.superseded_by IS NULL
    ORDER BY e.created_at DESC
    LIMIT #{FTS_CANDIDATES};
  SQL
end

def query_vector(vec_db, prompt)
  return [] if vec_db.nil?

  embedding = ollama_embed(prompt)
  return [] if embedding.nil?

  json_vec = JSON.generate(embedding)
  vec_results = sql_query(vec_db, <<~SQL, [json_vec])
    SELECT rowid, distance
    FROM entries_vec
    WHERE embedding MATCH ?
    ORDER BY distance
    LIMIT #{VECTOR_CANDIDATES};
  SQL

  return [] if vec_results.empty?

  vec_results = vec_results.reject { |r| r['distance'] > VECTOR_DISTANCE_THRESHOLD }
  return [] if vec_results.empty?

  dist_by_id = {}
  vec_results.each { |r| dist_by_id[r['rowid']] = r['distance'] }

  ids = vec_results.map { |r| r['rowid'] }
  placeholders = ids.map { '?' }.join(',')
  entries = sql_query(vec_db,
    "SELECT id, type, content, created_at FROM entries WHERE id IN (#{placeholders}) AND superseded_by IS NULL;",
    ids)

  entries.each { |e| e['distance'] = dist_by_id[e['id']] }
  entries.sort_by { |e| e['distance'] || Float::INFINITY }
end

def rrf_merge(fts_entries, vector_entries, k: RRF_K, limit: RRF_LIMIT)
  scores = Hash.new(0.0)
  entry_by_id = {}

  fts_entries.each_with_index do |entry, rank|
    id = entry['id']
    scores[id] += 1.0 / (k + rank + 1)
    entry_by_id[id] = entry
  end

  vector_entries.each_with_index do |entry, rank|
    id = entry['id']
    scores[id] += 1.0 / (k + rank + 1)
    entry_by_id[id] ||= entry
  end

  sorted = scores.sort_by { |_id, score| -score }
  sorted.first(limit).map { |id, _score| entry_by_id[id] }
end

def format_date(datetime_str)
  return nil if datetime_str.nil? || datetime_str.empty?
  datetime_str[0, 10] # YYYY-MM-DD from "YYYY-MM-DD HH:MM:SS"
rescue
  nil
end

def format_triples(triples)
  return '' if triples.empty?
  sorted = triples.sort_by { |t| t['valid_from'] || '' }.reverse
  lines = sorted.map do |t|
    since = format_date(t['valid_from'])
    since_str = since ? " (since #{since})" : ''
    "- #{t['subject']} → #{t['predicate']} → #{t['object']}#{since_str}"
  end
  "## Known facts\n\n#{lines.join("\n")}"
end

def format_entries(entries)
  return '' if entries.empty?
  lines = entries.map do |e|
    date = format_date(e['created_at'])
    prefix = date ? "#{date} #{e['type']}" : e['type']
    "- [#{prefix}] #{e['content']}"
  end
  "## Memory entries\n\n#{lines.join("\n")}"
end

def cross_encoder_rerank(prompt, entries)
  return entries if entries.empty?

  scored = entries.map { |entry|
    score = rerank_score(prompt, entry['content'])
    entry.merge('rerank_score' => score)
  }
  scored.select { |e| e['rerank_score'] > RERANK_THRESHOLD }
        .sort_by { |e| -e['rerank_score'] }
        .first(RERANK_LIMIT)
rescue => e
  spill_log(:error, "rerank failed: #{e.message}")
  entries.first(RERANK_LIMIT)
end

def rerank_score(prompt, document)
  uri = URI("#{OLLAMA_HOST}/api/chat")
  req = Net::HTTP::Post.new(uri, 'Content-Type' => 'application/json')

  rerank_prompt = "Judge whether the Document is relevant to the Query. " \
    "Answer exactly \"yes\" or \"no\", nothing else.\n\n" \
    "Query: #{prompt}\n\nDocument: #{document}"

  req.body = JSON.generate({
    model: RERANK_MODEL,
    messages: [{ role: 'user', content: rerank_prompt }],
    stream: false,
    logprobs: true,
    top_logprobs: 10,
    options: { temperature: 0.0, num_predict: 1 }
  })

  response = Net::HTTP.start(uri.hostname, uri.port) do |http|
    http.read_timeout = 30
    http.request(req)
  end

  return 0.0 unless response.is_a?(Net::HTTPSuccess)

  data = JSON.parse(response.body)
  top = data.dig('logprobs', 0, 'top_logprobs')
  return 0.0 unless top

  yes_lp = nil
  no_lp = nil
  top.each do |t|
    token = t['token'].strip.downcase
    yes_lp = t['logprob'] if token == 'yes' && yes_lp.nil?
    no_lp = t['logprob'] if token == 'no' && no_lp.nil?
  end

  if yes_lp && no_lp
    yes_p = Math.exp(yes_lp)
    no_p = Math.exp(no_lp)
    yes_p / (yes_p + no_p)
  elsif yes_lp
    1.0
  else
    0.0
  end
rescue Errno::ECONNREFUSED
  spill_log(:warn, 'ollama not running (connection refused), skipping rerank')
  0.0
rescue => e
  spill_log(:error, "rerank_score failed: #{e.message}")
  0.0
end

def truncate(text, max_chars)
  return text if text.length <= max_chars
  text[0, max_chars] + "\n\n[truncated — #{TOKEN_BUDGET} token budget exceeded]"
end

def update_last_retrieved(db, entry_ids)
  return if entry_ids.empty?
  placeholders = entry_ids.map { '?' }.join(',')
  sql_exec(db,
    "UPDATE entries SET last_retrieved_at = datetime('now') WHERE id IN (#{placeholders});",
    entry_ids)
end

def cmd_retrieve
  raw = $stdin.read
  exit 0 if raw.nil? || raw.strip.empty?
  parsed = JSON.parse(raw) rescue nil
  prompt = if parsed.is_a?(Hash)
             parsed['prompt'] || parsed.dig('tool_input', 'command') || raw
           else
             raw
           end
  exit 0 unless db_exists?
  prompt = prompt.strip

  keywords = extract_keywords(prompt)
  exit 0 if keywords.empty?

  db = open_db
  run_migrations(db)
  vec_db = open_db_vec
  if vec_db
    run_migrations(vec_db)
  end

  channel = ENV['CRIB_CHANNEL']
  case channel
  when 'triples'
    triples = query_triples(db, keywords)
    entries = []
    vector_entries = []
  when 'fts'
    triples = []
    entries = query_fts(db, keywords)
    vector_entries = []
  when 'vector'
    triples = []
    entries = []
    vector_entries = query_vector(vec_db, prompt)
  else
    triples = query_triples(db, keywords)
    entries = query_fts(db, keywords)
    vector_entries = query_vector(vec_db, prompt)
  end

  all_entries = if entries.any? && vector_entries.any?
                  rrf_merge(entries, vector_entries)
                elsif entries.any?
                  entries.first(RRF_LIMIT)
                elsif vector_entries.any?
                  vector_entries.first(RRF_LIMIT)
                else
                  []
                end

  all_entries = cross_encoder_rerank(prompt, all_entries) if all_entries.any?

  # Track access times on returned entries
  retrieved_ids = all_entries.map { |e| e['id'] }.compact
  update_last_retrieved(db, retrieved_ids)

  db.close
  vec_db&.close

  triples_text = format_triples(triples)
  entries_text = format_entries(all_entries)

  combined = [triples_text, entries_text].reject(&:empty?).join("\n\n")
  exit 0 if combined.empty?

  combined = truncate(combined, CHAR_BUDGET)

  context_time = Time.now.utc.strftime('%Y-%m-%dT%H:%M:%SZ')
  puts "<memory context_time=\"#{context_time}\">\n#{combined}\n</memory>"
end

# ---------------------------------------------------------------------------
# Maintain
# ---------------------------------------------------------------------------

def classify_contradiction(content_a, content_b)
  classify_bin = File.join(SCREEN_HOME, 'bin', 'classify')
  return nil unless File.executable?(classify_bin)

  input = JSON.generate({
    'condition' => 'These two statements contradict each other (they make incompatible claims about the same topic).',
    'content' => "Statement A: #{content_a}\n\nStatement B: #{content_b}"
  })

  stdout, status = Open3.capture2(classify_bin, stdin_data: input)
  return nil unless status.success?
  stdout.strip.downcase
rescue Errno::ENOENT
  spill_log(:warn, 'classify not found, skipping contradiction detection')
  nil
rescue => e
  spill_log(:error, "classify failed: #{e.message}")
  nil
end

def cmd_maintain
  unless db_exists?
    puts JSON.generate({ contradictions_found: 0, corrections_linked: 0, stale_count: 0, supersessions_applied: 0 })
    exit 0
  end

  db = open_db
  run_migrations(db)

  contradictions_found = 0
  corrections_linked = 0
  supersessions_applied = 0
  stale_count = 0

  # --- Contradiction detection ---
  # Compare recent entries pairwise via FTS5 similarity. For each pair
  # with matching terms, call classify to confirm a contradiction.
  begin
    recent = sql_query(db,
      "SELECT id, content FROM entries WHERE superseded_by IS NULL ORDER BY created_at DESC LIMIT 50;")

    recent.each_with_index do |entry_a, i|
      keywords_a = extract_keywords(entry_a['content'])
      next if keywords_a.empty?

      fts_query = keywords_a.first(5).join(' OR ')
      similar = sql_query(db, <<~SQL, [fts_query, entry_a['id']])
        SELECT e.id, e.content, e.created_at
        FROM entries e
        JOIN entries_fts f ON e.id = f.rowid
        WHERE entries_fts MATCH ?
          AND e.id != ?
          AND e.superseded_by IS NULL
        ORDER BY e.created_at DESC
        LIMIT 5;
      SQL

      similar.each do |entry_b|
        answer = classify_contradiction(entry_a['content'], entry_b['content'])
        next unless answer == 'yes'

        contradictions_found += 1

        # Supersede the older entry
        older, newer = [entry_a, entry_b].sort_by { |e| e['created_at'] || '' }
        sql_exec(db,
          "UPDATE entries SET superseded_by = ? WHERE id = ? AND superseded_by IS NULL;",
          [newer['id'], older['id']])
        supersessions_applied += 1
      end
    end
  rescue => e
    spill_log(:error, "contradiction detection failed: #{e.message}")
  end

  # --- Correction linking ---
  # Find type=correction entries without a supersession link, then find
  # the original entry being corrected via vector search.
  begin
    corrections = sql_query(db,
      "SELECT id, content FROM entries WHERE type = 'correction' AND superseded_by IS NULL;")

    vec_db = open_db_vec

    corrections.each do |correction|
      embedding = ollama_embed(correction['content'])
      next if embedding.nil? || vec_db.nil?

      json_vec = JSON.generate(embedding)
      candidates = sql_query(vec_db, <<~SQL, [json_vec, correction['id']])
        SELECT rowid, distance
        FROM entries_vec
        WHERE embedding MATCH ?
        ORDER BY distance
        LIMIT 5;
      SQL

      candidates = candidates.reject { |c| c['rowid'] == correction['id'] }
      next if candidates.empty?

      # Find the closest non-correction entry that is not already superseded
      candidate_ids = candidates.map { |c| c['rowid'] }
      placeholders = candidate_ids.map { '?' }.join(',')
      originals = sql_query(db,
        "SELECT id FROM entries WHERE id IN (#{placeholders}) AND type != 'correction' AND superseded_by IS NULL;",
        candidate_ids)

      next if originals.empty?

      # Supersede the closest original
      original_id = originals.first['id']
      sql_exec(db,
        "UPDATE entries SET superseded_by = ? WHERE id = ? AND superseded_by IS NULL;",
        [correction['id'], original_id])
      corrections_linked += 1
      supersessions_applied += 1
    end

    vec_db&.close
  rescue => e
    spill_log(:error, "correction linking failed: #{e.message}")
  end

  # --- Staleness tracking ---
  begin
    stale = sql_query(db, <<~SQL, [STALENESS_DAYS])
      SELECT COUNT(*) AS cnt FROM entries
      WHERE superseded_by IS NULL
        AND last_retrieved_at IS NOT NULL
        AND last_retrieved_at < datetime('now', '-' || ? || ' days');
    SQL
    stale_count = stale.first['cnt'] if stale.any?
  rescue => e
    spill_log(:error, "staleness check failed: #{e.message}")
  end

  db.close

  result = {
    contradictions_found: contradictions_found,
    corrections_linked: corrections_linked,
    stale_count: stale_count,
    supersessions_applied: supersessions_applied
  }

  spill_log(:info, "maintain complete", **result)
  puts JSON.generate(result)
  exit 0
end

# ---------------------------------------------------------------------------
# Init / Schema
# ---------------------------------------------------------------------------

def cmd_init
  init_db
  spill_log(:info, "initialized #{DB_PATH}")
end

def cmd_doctor
  report = {}

  # Ruby
  report['ruby'] = { 'version' => RUBY_VERSION, 'ok' => true }

  # sqlite3 gem
  report['sqlite3_gem'] = {
    'version' => SQLite3::VERSION,
    'ok' => true
  }

  # sqlite-vec extension
  begin
    vec_db = SQLite3::Database.new(':memory:')
    vec_db.enable_load_extension(true)
    vec_db.load_extension(VEC_EXTENSION)
    vec_version = vec_db.execute('SELECT vec_version();').first.first
    vec_db.close
    report['sqlite_vec'] = {
      'path' => VEC_EXTENSION,
      'version' => vec_version,
      'ok' => true
    }
  rescue => e
    report['sqlite_vec'] = {
      'path' => VEC_EXTENSION,
      'version' => nil,
      'ok' => false,
      'error' => e.message
    }
  end

  # ollama
  ollama_out, ollama_status = Open3.capture2('ollama', '--version')
  report['ollama'] = {
    'version' => ollama_status.success? ? ollama_out.strip : nil,
    'ok' => ollama_status.success?
  }

  # ollama reachable
  begin
    uri = URI("#{OLLAMA_HOST}/api/tags")
    response = Net::HTTP.get_response(uri)
    models = []
    if response.is_a?(Net::HTTPSuccess)
      data = JSON.parse(response.body)
      models = (data['models'] || []).map { |m| m['name'] }
    end
    report['ollama_api'] = { 'host' => OLLAMA_HOST, 'ok' => response.is_a?(Net::HTTPSuccess) }
  rescue => e
    report['ollama_api'] = { 'host' => OLLAMA_HOST, 'ok' => false, 'error' => e.message }
    models = []
  end

  # Models
  [EXTRACTION_MODEL, RERANK_MODEL, EMBEDDING_MODEL].uniq.each do |model|
    present = models.any? { |m| m.start_with?(model) }
    report["model:#{model}"] = { 'ok' => present }
  end

  # Database
  report['database'] = {
    'path' => DB_PATH,
    'exists' => File.exist?(DB_PATH)
  }

  # Overall
  report['ok'] = report.values.all? { |v| v.is_a?(Hash) ? v['ok'] != false : true }

  puts JSON.pretty_generate(report)
  exit(report['ok'] ? 0 : 1)
end

# ---------------------------------------------------------------------------
# Dispatch
# ---------------------------------------------------------------------------

def usage
  $stderr.puts <<~USAGE
    Usage: crib <command>

    Commands:
      write     Store text and extracted fact triples (stdin)
      retrieve  Query memory for relevant context (stdin)
      init      Initialize the database
      doctor    Check prerequisites and report status (JSON)
      maintain  Run contradiction detection, correction linking, and staleness (JSON)

    Environment:
      CRIB_DB   Path to SQLite database (default: .state/crib/crib.db)
  USAGE
  exit 1
end

command = ARGV.shift
case command
when 'write'    then cmd_write
when 'retrieve' then cmd_retrieve
when 'init'     then cmd_init
when 'doctor'   then cmd_doctor
when 'maintain' then cmd_maintain
else usage
end
