#!/usr/bin/env ruby
# frozen_string_literal: true

# crib — memory storage and retrieval for AI agents
# https://github.com/bioneural/crib
# MIT License — Copyright (c) 2026 Kerry Ivan Kurian
#
# A single-file memory system: fact triples, full-text search, and vector
# embeddings in one SQLite database. Write stores text and extracts triples.
# Retrieve queries all channels and returns what's relevant for a given prompt.
#
# Subcommands:
#
#   crib write     Store text and extracted fact triples
#   crib retrieve  Query memory and return relevant context
#   crib init      Initialize the database (run automatically on first write)
#   crib doctor    Check all prerequisites and report status (JSON on stdout)
#
# Usage:
#   echo "Chose SQLite for storage" | bin/crib write
#   echo "type=decision Chose SQLite" | bin/crib write
#   echo "what database did we choose?" | bin/crib retrieve
#   bin/crib init
#   bin/crib doctor
#
# Environment:
#   CRIB_DB       Path to the SQLite database file.
#                 Default: .state/crib/crib.db (relative to cwd)
#   CRIB_CHANNEL  Isolate a single retrieval channel: triples, fts, or vector.
#                 When unset, all three channels run (default behavior).
#   CRIB_DISTANCE_METRIC
#                 Distance metric for vector search: cosine or L2.
#                 Default: cosine (matches nomic-embed-text optimization).
#                 Changing this on an existing database requires dropping and
#                 recreating the entries_vec table (embeddings are re-inserted
#                 automatically on next write, but existing entries need
#                 re-embedding via: crib reindex).
#   CRIB_VECTOR_THRESHOLD
#                 Maximum vector distance for results. Entries beyond this
#                 distance are filtered out. Default: 0.5 (cosine).
#                 If using L2 metric, a threshold of 1.0 is equivalent.
#   CRIB_RRF_K    Reciprocal Rank Fusion smoothing constant. Higher values
#                 reduce the influence of high-ranked results. Default: 60
#                 (per Cormack et al. 2009).
#   CRIB_RERANK_THRESHOLD
#                 Minimum rerank score for results. Entries at or below this
#                 score are filtered out. Default: 0.0 (filters entries the
#                 reranker scored as completely irrelevant — exact zeros).
#                 Noise queries produce exact-zero scores for all candidates,
#                 enabling empty results. Set higher (e.g. 0.5) for strict
#                 relevance filtering, or -1 to disable.
#
# Output:
#   write     — status on stderr, nothing on stdout
#   retrieve  — <memory context_time="...">...</memory> on stdout (empty = nothing relevant)
#   init      — status on stderr
#
# Dependencies: ruby (stdlib only), sqlite3 (CLI), sqlite-vec (extension), ollama
#
# Behavior:
#   write:
#     1. Read text from stdin
#     2. Parse optional type= prefix (decision, correction, note, error)
#     3. Initialize database if needed
#     4. Store entry with FTS5 indexing
#     5. Extract fact triples via ollama
#     6. Store entities and relations (consolidation-on-write: supersedes stale facts)
#     7. Generate embedding via ollama and store in vec0 table
#
#   retrieve:
#     1. Read prompt from stdin
#     2. Extract keywords
#     3. Query three channels: triples (SQL), full-text (FTS5), vectors (sqlite-vec)
#     4. Merge and deduplicate
#     5. Rerank with ollama if over token budget
#     6. Wrap in <memory> tags and output to stdout
#     7. Exit 0 always (fail-open)
#
# Schema: see the SCHEMA constant below or the README.

require 'json'
require 'open3'
require 'net/http'
require 'fileutils'
require 'uri'

SPILL_HOME = ENV['SPILL_HOME'] || File.expand_path('../../spill', __dir__)
if File.directory?(SPILL_HOME)
  require File.join(SPILL_HOME, 'lib', 'spill')
  Spill.configure(tool: 'crib')
end

# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------

DB_PATH = ENV.fetch('CRIB_DB') { File.join(Dir.pwd, '.state', 'crib', 'crib.db') }
EMBEDDING_MODEL = ENV.fetch('CRIB_EMBEDDING_MODEL', 'nomic-embed-text')
EXTRACTION_MODEL = ENV.fetch('CRIB_EXTRACTION_MODEL', 'gemma3:1b')
RERANK_MODEL = ENV.fetch('CRIB_RERANK_MODEL', 'gemma3:1b')
OLLAMA_HOST = ENV.fetch('OLLAMA_HOST', 'http://localhost:11434')
EMBEDDING_DIM = 768 # nomic-embed-text output dimension
DISTANCE_METRIC = ENV.fetch('CRIB_DISTANCE_METRIC', 'cosine')
VECTOR_DISTANCE_THRESHOLD = ENV.fetch('CRIB_VECTOR_THRESHOLD', '0.5').to_f
RRF_K = ENV.fetch('CRIB_RRF_K', '60').to_i
RRF_LIMIT = 20
RERANK_LIMIT = 10
RERANK_THRESHOLD = ENV.fetch('CRIB_RERANK_THRESHOLD', '0.0').to_f
FTS_CANDIDATES = 20
VECTOR_CANDIDATES = 20

# sqlite3 binary: macOS system sqlite3 has extension loading disabled.
# Homebrew's sqlite3 supports .load. Auto-detect or override via env var.
SQLITE3 = ENV.fetch('CRIB_SQLITE3') {
  brew_path = '/opt/homebrew/opt/sqlite/bin/sqlite3'
  File.executable?(brew_path) ? brew_path : 'sqlite3'
}

# sqlite-vec extension: auto-detect via Python package or override via env var.
VEC_EXTENSION = ENV.fetch('CRIB_VEC_EXTENSION') {
  detected, status = Open3.capture2('python3', '-c', 'import sqlite_vec; print(sqlite_vec.loadable_path())')
  status.success? ? detected.strip : 'vec0'
}
TOKEN_BUDGET = 2000
CHAR_BUDGET = TOKEN_BUDGET * 4

# ---------------------------------------------------------------------------
# Schema
# ---------------------------------------------------------------------------

SCHEMA = <<~SQL
  -- Fact triples (structured recall)
  CREATE TABLE IF NOT EXISTS entities (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT NOT NULL UNIQUE,
    type TEXT NOT NULL,
    created_at TEXT DEFAULT (datetime('now'))
  );

  CREATE TABLE IF NOT EXISTS relations (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    subject_id INTEGER NOT NULL REFERENCES entities(id),
    predicate TEXT NOT NULL,
    object_id INTEGER NOT NULL REFERENCES entities(id),
    valid_from TEXT DEFAULT (datetime('now')),
    valid_until TEXT,
    created_at TEXT DEFAULT (datetime('now'))
  );

  CREATE TABLE IF NOT EXISTS sources (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    relation_id INTEGER NOT NULL REFERENCES relations(id),
    raw_text TEXT NOT NULL,
    timestamp TEXT DEFAULT (datetime('now'))
  );

  -- Full-text entries (unstructured recall)
  CREATE TABLE IF NOT EXISTS entries (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    type TEXT NOT NULL,
    content TEXT NOT NULL,
    embedding BLOB,
    created_at TEXT DEFAULT (datetime('now'))
  );

  CREATE VIRTUAL TABLE IF NOT EXISTS entries_fts USING fts5(
    content,
    content='entries',
    content_rowid='id',
    tokenize='porter unicode61'
  );

  CREATE TRIGGER IF NOT EXISTS entries_ai AFTER INSERT ON entries BEGIN
    INSERT INTO entries_fts(rowid, content) VALUES (new.id, new.content);
  END;

  CREATE TRIGGER IF NOT EXISTS entries_ad AFTER DELETE ON entries BEGIN
    INSERT INTO entries_fts(entries_fts, rowid, content) VALUES('delete', old.id, old.content);
  END;

  CREATE TRIGGER IF NOT EXISTS entries_au AFTER UPDATE ON entries BEGIN
    INSERT INTO entries_fts(entries_fts, rowid, content) VALUES('delete', old.id, old.content);
    INSERT INTO entries_fts(rowid, content) VALUES (new.id, new.content);
  END;
SQL

# Vec0 table must be created separately because it requires the sqlite-vec extension
VEC_SCHEMA = <<~SQL
  CREATE VIRTUAL TABLE IF NOT EXISTS entries_vec USING vec0(
    embedding float[#{EMBEDDING_DIM}] distance_metric=#{DISTANCE_METRIC}
  );
SQL

# ---------------------------------------------------------------------------
# Shared helpers
# ---------------------------------------------------------------------------

def escape_sql(str)
  str.gsub("'", "''")
end

def sql_exec(query)
  stdout, stderr, status = Open3.capture3(SQLITE3, DB_PATH, stdin_data: query)
  unless status.success?
    defined?(Spill) ? Spill.error("sqlite3 error: #{stderr.strip}") : $stderr.puts("crib: sqlite3 error: #{stderr.strip}")
    return nil
  end
  stdout.strip
end

def sql_query_json(query)
  stdout, stderr, status = Open3.capture3(SQLITE3, '-json', DB_PATH, stdin_data: query)
  unless status.success?
    defined?(Spill) ? Spill.error("sqlite3 error: #{stderr.strip}") : $stderr.puts("crib: sqlite3 error: #{stderr.strip}")
    return []
  end
  return [] if stdout.strip.empty?
  JSON.parse(stdout)
rescue JSON::ParserError => e
  defined?(Spill) ? Spill.error("JSON parse error: #{e.message}") : $stderr.puts("crib: JSON parse error: #{e.message}")
  []
end

def sql_exec_vec(query)
  stdout, stderr, status = Open3.capture3(
    SQLITE3, '-cmd', ".load #{VEC_EXTENSION}", DB_PATH,
    stdin_data: query
  )
  unless status.success?
    defined?(Spill) ? Spill.error("sqlite3+vec error: #{stderr.strip}") : $stderr.puts("crib: sqlite3+vec error: #{stderr.strip}")
    return nil
  end
  stdout.strip
end

def sql_query_json_vec(query)
  stdout, stderr, status = Open3.capture3(
    SQLITE3, '-json', '-cmd', ".load #{VEC_EXTENSION}", DB_PATH,
    stdin_data: query
  )
  unless status.success?
    defined?(Spill) ? Spill.error("sqlite3+vec error: #{stderr.strip}") : $stderr.puts("crib: sqlite3+vec error: #{stderr.strip}")
    return []
  end
  return [] if stdout.strip.empty?
  JSON.parse(stdout)
rescue JSON::ParserError => e
  defined?(Spill) ? Spill.error("JSON parse error: #{e.message}") : $stderr.puts("crib: JSON parse error: #{e.message}")
  []
end

def call_ollama(prompt, model)
  stdout, stderr, status = Open3.capture3('ollama', 'run', model, stdin_data: prompt)
  unless status.success?
    defined?(Spill) ? Spill.error("ollama error (#{model}): #{stderr.strip}") : $stderr.puts("crib: ollama error (#{model}): #{stderr.strip}")
    return nil
  end
  stdout.strip
rescue Errno::ENOENT
  defined?(Spill) ? Spill.warn('ollama not found') : $stderr.puts('crib: ollama not found')
  nil
rescue => e
  defined?(Spill) ? Spill.error("ollama failed: #{e.message}") : $stderr.puts("crib: ollama failed: #{e.message}")
  nil
end

def db_exists?
  File.exist?(DB_PATH)
end

def ensure_db_dir
  dir = File.dirname(DB_PATH)
  FileUtils.mkdir_p(dir) unless Dir.exist?(dir)
end

def init_db
  ensure_db_dir
  sql_exec(SCHEMA)
  sql_exec_vec(VEC_SCHEMA)
end

def ollama_embed(text)
  uri = URI("#{OLLAMA_HOST}/api/embed")
  req = Net::HTTP::Post.new(uri, 'Content-Type' => 'application/json')
  req.body = JSON.generate({ model: EMBEDDING_MODEL, input: text })

  response = Net::HTTP.start(uri.hostname, uri.port) do |http|
    http.read_timeout = 30
    http.request(req)
  end

  unless response.is_a?(Net::HTTPSuccess)
    defined?(Spill) ? Spill.error("ollama embed error: HTTP #{response.code}") : $stderr.puts("crib: ollama embed error: HTTP #{response.code}")
    return nil
  end

  data = JSON.parse(response.body)
  data['embeddings']&.first
rescue Errno::ECONNREFUSED
  defined?(Spill) ? Spill.warn('ollama not running (connection refused)') : $stderr.puts('crib: ollama not running (connection refused)')
  nil
rescue => e
  defined?(Spill) ? Spill.error("ollama embed failed: #{e.message}") : $stderr.puts("crib: ollama embed failed: #{e.message}")
  nil
end

# ---------------------------------------------------------------------------
# Write
# ---------------------------------------------------------------------------

def store_entry(type, content)
  result = sql_exec(<<~SQL)
    INSERT INTO entries(type, content) VALUES ('#{escape_sql(type)}', '#{escape_sql(content)}');
    SELECT last_insert_rowid();
  SQL
  result&.to_i
end

def extract_triples(text)
  prompt = <<~PROMPT
    Extract factual relationships from the following text as JSON.
    Return a JSON array of objects, each with "subject", "predicate", "object", and "subject_type", "object_type" fields.
    Subject and object are entity names. Predicate is the relationship.
    Types are short categories like: person, project, tool, concept, decision, date, value.
    If no clear facts can be extracted, return an empty array: []
    Return ONLY valid JSON. No explanation.

    Text: #{text}
  PROMPT

  result = call_ollama(prompt, EXTRACTION_MODEL)
  return [] if result.nil? || result.empty?

  json_str = result[/\[.*\]/m]
  return [] if json_str.nil?

  JSON.parse(json_str)
rescue JSON::ParserError
  defined?(Spill) ? Spill.error('failed to parse triple extraction response') : $stderr.puts('crib: failed to parse triple extraction response')
  []
end

def find_or_create_entity(name, type)
  existing = sql_query_json("SELECT id FROM entities WHERE name = '#{escape_sql(name)}' LIMIT 1;")
  return existing.first['id'] unless existing.empty?

  result = sql_exec(<<~SQL)
    INSERT INTO entities(name, type) VALUES ('#{escape_sql(name)}', '#{escape_sql(type)}');
    SELECT last_insert_rowid();
  SQL
  result&.to_i
end

def store_triple(subject_name, subject_type, predicate, object_name, object_type, raw_text)
  subject_id = find_or_create_entity(subject_name, subject_type)
  object_id = find_or_create_entity(object_name, object_type)
  return if subject_id.nil? || object_id.nil?

  # Consolidation: supersede existing relations with same subject+predicate
  sql_exec(<<~SQL)
    UPDATE relations SET valid_until = datetime('now')
    WHERE subject_id = #{subject_id}
      AND predicate = '#{escape_sql(predicate)}'
      AND valid_until IS NULL;
  SQL

  relation_id = sql_exec(<<~SQL)&.to_i
    INSERT INTO relations(subject_id, predicate, object_id)
    VALUES (#{subject_id}, '#{escape_sql(predicate)}', #{object_id});
    SELECT last_insert_rowid();
  SQL
  return if relation_id.nil?

  sql_exec("INSERT INTO sources(relation_id, raw_text) VALUES (#{relation_id}, '#{escape_sql(raw_text)}');")
end

def generate_embedding(text)
  ollama_embed(text)
end

def store_embedding(entry_id, embedding)
  return if embedding.nil? || entry_id.nil?
  json_vec = JSON.generate(embedding)
  sql_exec_vec("INSERT INTO entries_vec(rowid, embedding) VALUES (#{entry_id}, '#{escape_sql(json_vec)}');")
end

def cmd_write
  text = $stdin.read
  if text.nil? || text.strip.empty?
    defined?(Spill) ? Spill.error('empty input') : $stderr.puts('crib: empty input')
    exit 1
  end
  text = text.strip

  # Parse optional type prefix
  type = 'note'
  if text.match?(/\Atype=\w+\s/)
    type = text[/\Atype=(\w+)/, 1]
    text = text.sub(/\Atype=\w+\s+/, '')
  end

  init_db

  entry_id = store_entry(type, text)
  if entry_id.nil?
    defined?(Spill) ? Spill.error('failed to store entry') : $stderr.puts('crib: failed to store entry')
    exit 1
  end

  triples = extract_triples(text)
  triples.each do |triple|
    store_triple(
      triple['subject'] || 'unknown',
      triple['subject_type'] || 'concept',
      triple['predicate'] || 'related_to',
      triple['object'] || 'unknown',
      triple['object_type'] || 'concept',
      text
    )
  end

  embedding = generate_embedding(text)
  store_embedding(entry_id, embedding)

  defined?(Spill) ? Spill.info("stored entry ##{entry_id} (#{type}), #{triples.length} triples extracted") : $stderr.puts("crib: stored entry ##{entry_id} (#{type}), #{triples.length} triples extracted")
end

# ---------------------------------------------------------------------------
# Retrieve
# ---------------------------------------------------------------------------

STOP_WORDS = %w[a an the is are was were be been being have has had do does did
                will would shall should may might can could of in to for on with
                at by from as into about between through during before after
                and or but not no nor so yet both either neither each every all
                any few more most other some such this that these those
                i me my we our you your he him his she her it its they them their
                what which who whom how when where why if then else
                just also very too quite rather really].freeze

def extract_keywords(prompt)
  words = prompt.downcase.gsub(/[^\w\s]/, '').split
  words.reject { |w| w.length < 3 || STOP_WORDS.include?(w) }.uniq
end

def query_triples(keywords)
  return [] if keywords.empty?

  like_clauses = keywords.map { |k| "name LIKE '%#{escape_sql(k)}%'" }.join(' OR ')

  sql_query_json(<<~SQL)
    SELECT
      s.name AS subject,
      r.predicate,
      o.name AS object,
      r.valid_from,
      r.valid_until
    FROM relations r
    JOIN entities s ON r.subject_id = s.id
    JOIN entities o ON r.object_id = o.id
    WHERE r.valid_until IS NULL
      AND (s.id IN (SELECT id FROM entities WHERE #{like_clauses})
           OR o.id IN (SELECT id FROM entities WHERE #{like_clauses}))
    ORDER BY r.created_at DESC
    LIMIT 20;
  SQL
end

def query_fts(keywords)
  return [] if keywords.empty?

  fts_query = keywords.join(' OR ')

  sql_query_json(<<~SQL)
    SELECT e.id, e.type, e.content, e.created_at
    FROM entries e
    JOIN entries_fts f ON e.id = f.rowid
    WHERE entries_fts MATCH '#{escape_sql(fts_query)}'
    ORDER BY e.created_at DESC
    LIMIT #{FTS_CANDIDATES};
  SQL
end

def query_vector(prompt)
  embedding = ollama_embed(prompt)
  return [] if embedding.nil?

  json_vec = JSON.generate(embedding)
  vec_results = sql_query_json_vec(<<~SQL)
    SELECT rowid, distance
    FROM entries_vec
    WHERE embedding MATCH '#{escape_sql(json_vec)}'
    ORDER BY distance
    LIMIT #{VECTOR_CANDIDATES};
  SQL

  return [] if vec_results.empty?

  vec_results.reject! { |r| r['distance'] > VECTOR_DISTANCE_THRESHOLD }
  return [] if vec_results.empty?

  dist_by_id = {}
  vec_results.each { |r| dist_by_id[r['rowid']] = r['distance'] }

  ids = vec_results.map { |r| r['rowid'] }.join(',')
  entries = sql_query_json("SELECT id, type, content, created_at FROM entries WHERE id IN (#{ids});")

  entries.each { |e| e['distance'] = dist_by_id[e['id']] }
  entries.sort_by { |e| e['distance'] || Float::INFINITY }
end

def rrf_merge(fts_entries, vector_entries, k: RRF_K, limit: RRF_LIMIT)
  scores = Hash.new(0.0)
  entry_by_id = {}

  fts_entries.each_with_index do |entry, rank|
    id = entry['id']
    scores[id] += 1.0 / (k + rank + 1)
    entry_by_id[id] = entry
  end

  vector_entries.each_with_index do |entry, rank|
    id = entry['id']
    scores[id] += 1.0 / (k + rank + 1)
    entry_by_id[id] ||= entry
  end

  sorted = scores.sort_by { |_id, score| -score }
  sorted.first(limit).map { |id, _score| entry_by_id[id] }
end

def format_date(datetime_str)
  return nil if datetime_str.nil? || datetime_str.empty?
  datetime_str[0, 10] # YYYY-MM-DD from "YYYY-MM-DD HH:MM:SS"
rescue
  nil
end

def format_triples(triples)
  return '' if triples.empty?
  sorted = triples.sort_by { |t| t['valid_from'] || '' }.reverse
  lines = sorted.map do |t|
    since = format_date(t['valid_from'])
    since_str = since ? " (since #{since})" : ''
    "- #{t['subject']} → #{t['predicate']} → #{t['object']}#{since_str}"
  end
  "## Known facts\n\n#{lines.join("\n")}"
end

def format_entries(entries)
  return '' if entries.empty?
  lines = entries.map do |e|
    date = format_date(e['created_at'])
    prefix = date ? "#{date} #{e['type']}" : e['type']
    "- [#{prefix}] #{e['content']}"
  end
  "## Memory entries\n\n#{lines.join("\n")}"
end

def cross_encoder_rerank(prompt, entries)
  return entries if entries.empty?

  scored = entries.map { |entry|
    score = rerank_score(prompt, entry['content'])
    entry.merge('rerank_score' => score)
  }
  scored.select { |e| e['rerank_score'] > RERANK_THRESHOLD }
        .sort_by { |e| -e['rerank_score'] }
        .first(RERANK_LIMIT)
rescue => e
  defined?(Spill) ? Spill.error("rerank failed: #{e.message}") : $stderr.puts("crib: rerank failed: #{e.message}")
  entries.first(RERANK_LIMIT)
end

def rerank_score(prompt, document)
  uri = URI("#{OLLAMA_HOST}/api/chat")
  req = Net::HTTP::Post.new(uri, 'Content-Type' => 'application/json')

  rerank_prompt = "Judge whether the Document is relevant to the Query. " \
    "Answer exactly \"yes\" or \"no\", nothing else.\n\n" \
    "Query: #{prompt}\n\nDocument: #{document}"

  req.body = JSON.generate({
    model: RERANK_MODEL,
    messages: [{ role: 'user', content: rerank_prompt }],
    stream: false,
    logprobs: true,
    top_logprobs: 10,
    options: { temperature: 0.0, num_predict: 1 }
  })

  response = Net::HTTP.start(uri.hostname, uri.port) do |http|
    http.read_timeout = 30
    http.request(req)
  end

  return 0.0 unless response.is_a?(Net::HTTPSuccess)

  data = JSON.parse(response.body)
  top = data.dig('logprobs', 0, 'top_logprobs')
  return 0.0 unless top

  yes_lp = nil
  no_lp = nil
  top.each do |t|
    token = t['token'].strip.downcase
    yes_lp = t['logprob'] if token == 'yes' && yes_lp.nil?
    no_lp = t['logprob'] if token == 'no' && no_lp.nil?
  end

  if yes_lp && no_lp
    yes_p = Math.exp(yes_lp)
    no_p = Math.exp(no_lp)
    yes_p / (yes_p + no_p)
  elsif yes_lp
    1.0
  else
    0.0
  end
rescue Errno::ECONNREFUSED
  defined?(Spill) ? Spill.warn('ollama not running (connection refused), skipping rerank') : $stderr.puts('crib: ollama not running, skipping rerank')
  0.0
rescue => e
  defined?(Spill) ? Spill.error("rerank_score failed: #{e.message}") : $stderr.puts("crib: rerank_score failed: #{e.message}")
  0.0
end

def truncate(text, max_chars)
  return text if text.length <= max_chars
  text[0, max_chars] + "\n\n[truncated — #{TOKEN_BUDGET} token budget exceeded]"
end

def cmd_retrieve
  raw = $stdin.read
  exit 0 if raw.nil? || raw.strip.empty?
  parsed = JSON.parse(raw) rescue nil
  prompt = if parsed.is_a?(Hash)
             parsed['prompt'] || parsed.dig('tool_input', 'command') || raw
           else
             raw
           end
  exit 0 unless db_exists?
  prompt = prompt.strip

  keywords = extract_keywords(prompt)
  exit 0 if keywords.empty?

  channel = ENV['CRIB_CHANNEL']
  case channel
  when 'triples'
    triples = query_triples(keywords)
    entries = []
    vector_entries = []
  when 'fts'
    triples = []
    entries = query_fts(keywords)
    vector_entries = []
  when 'vector'
    triples = []
    entries = []
    vector_entries = query_vector(prompt)
  else
    triples = query_triples(keywords)
    entries = query_fts(keywords)
    vector_entries = query_vector(prompt)
  end

  all_entries = if entries.any? && vector_entries.any?
                  rrf_merge(entries, vector_entries)
                elsif entries.any?
                  entries.first(RRF_LIMIT)
                elsif vector_entries.any?
                  vector_entries.first(RRF_LIMIT)
                else
                  []
                end

  all_entries = cross_encoder_rerank(prompt, all_entries) if all_entries.any?

  triples_text = format_triples(triples)
  entries_text = format_entries(all_entries)

  combined = [triples_text, entries_text].reject(&:empty?).join("\n\n")
  exit 0 if combined.empty?

  combined = truncate(combined, CHAR_BUDGET)

  context_time = Time.now.utc.strftime('%Y-%m-%dT%H:%M:%SZ')
  puts "<memory context_time=\"#{context_time}\">\n#{combined}\n</memory>"
end

# ---------------------------------------------------------------------------
# Init / Schema
# ---------------------------------------------------------------------------

def cmd_init
  init_db
  defined?(Spill) ? Spill.info("initialized #{DB_PATH}") : $stderr.puts("crib: initialized #{DB_PATH}")
end

def cmd_doctor
  report = {}

  # Ruby
  report['ruby'] = { 'version' => RUBY_VERSION, 'ok' => true }

  # sqlite3 binary
  sqlite_out, sqlite_status = Open3.capture2(SQLITE3, '--version')
  report['sqlite3'] = {
    'path' => SQLITE3,
    'version' => sqlite_status.success? ? sqlite_out.strip.split.first : nil,
    'ok' => sqlite_status.success?
  }

  # sqlite-vec extension
  vec_out, vec_status = Open3.capture2(
    SQLITE3, '-cmd', ".load #{VEC_EXTENSION}", ':memory:',
    stdin_data: 'SELECT vec_version();'
  )
  report['sqlite_vec'] = {
    'path' => VEC_EXTENSION,
    'version' => vec_status.success? ? vec_out.strip : nil,
    'ok' => vec_status.success?
  }

  # ollama
  ollama_out, ollama_status = Open3.capture2('ollama', '--version')
  report['ollama'] = {
    'version' => ollama_status.success? ? ollama_out.strip : nil,
    'ok' => ollama_status.success?
  }

  # ollama reachable
  begin
    uri = URI("#{OLLAMA_HOST}/api/tags")
    response = Net::HTTP.get_response(uri)
    models = []
    if response.is_a?(Net::HTTPSuccess)
      data = JSON.parse(response.body)
      models = (data['models'] || []).map { |m| m['name'] }
    end
    report['ollama_api'] = { 'host' => OLLAMA_HOST, 'ok' => response.is_a?(Net::HTTPSuccess) }
  rescue => e
    report['ollama_api'] = { 'host' => OLLAMA_HOST, 'ok' => false, 'error' => e.message }
    models = []
  end

  # Models
  [EXTRACTION_MODEL, RERANK_MODEL, EMBEDDING_MODEL].uniq.each do |model|
    present = models.any? { |m| m.start_with?(model) }
    report["model:#{model}"] = { 'ok' => present }
  end

  # Database
  report['database'] = {
    'path' => DB_PATH,
    'exists' => File.exist?(DB_PATH)
  }

  # Overall
  report['ok'] = report.values.all? { |v| v.is_a?(Hash) ? v['ok'] != false : true }

  puts JSON.pretty_generate(report)
  exit(report['ok'] ? 0 : 1)
end

# ---------------------------------------------------------------------------
# Dispatch
# ---------------------------------------------------------------------------

def usage
  $stderr.puts <<~USAGE
    Usage: crib <command>

    Commands:
      write     Store text and extracted fact triples (stdin)
      retrieve  Query memory for relevant context (stdin)
      init      Initialize the database
      doctor    Check prerequisites and report status (JSON)

    Environment:
      CRIB_DB   Path to SQLite database (default: memory/crib.db)
  USAGE
  exit 1
end

command = ARGV.shift
case command
when 'write'    then cmd_write
when 'retrieve' then cmd_retrieve
when 'init'     then cmd_init
when 'doctor'   then cmd_doctor
else usage
end
